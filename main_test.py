pip install -r requirements.txt

from kiwipiepy import Kiwi
from kiwipiepy.utils import Stopwords
import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.retrievers import BM25Retriever
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
import gridfs
import os
import tempfile
import pickle
import streamlit as st
from dotenv import load_dotenv
import os
from langchain_teddynote import logging
import asyncio
import uuid

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
uri = "mongodb+srv://yiji:yiji1214@cluster0.v1ig9.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
client = MongoClient(uri, server_api=ServerApi('1'))
embeddings_model = OpenAIEmbeddings(model = 'text-embedding-3-small', api_key = api_key)

logging.langsmith(project_name="Hongdae_restaurants")
# logging.langsmith("Hongdae_restaurants", set_enable=False) # Disable logging

db = FAISS(
    embedding_function=embeddings_model,
    index=faiss.IndexFlatL2(1536),
    docstore=InMemoryDocstore(),
    index_to_docstore_id={},
)

db_m = client["Hongdae"]
faiss = gridfs.GridFS(db_m)

def create_virtual_folder_with_files():
    # ì„ì‹œ í´ë” ìƒì„±
    temp_folder = tempfile.TemporaryDirectory()
    folder_path = temp_folder.name  # ì„ì‹œ í´ë” ê²½ë¡œ

    # MongoDBì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    def download_file_from_mongodb(filename, output_path):
        file = faiss.find_one({"filename": filename})
        if file:
            with open(output_path, "wb") as f:
                f.write(file.read())

    # í•„ìš”í•œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    download_file_from_mongodb("index.faiss", os.path.join(folder_path, "index.faiss"))
    download_file_from_mongodb("index.pkl", os.path.join(folder_path, "index.pkl"))

    # ì„ì‹œ í´ë” ê²½ë¡œ ë°˜í™˜ (í´ë”ëŠ” í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì‚­ì œë¨)
    return folder_path, temp_folder

# ì„ì‹œ í´ë” ìƒì„± ë° íŒŒì¼ ì €ì¥
folder_path, temp_folder = create_virtual_folder_with_files()

db = db.load_local(folder_path, embeddings=embeddings_model, allow_dangerous_deserialization=True)
docs = db.docstore._dict.values()

kiwi = Kiwi(typos="basic", model_type='sbg')
stopwords = Stopwords()
stopwords.remove(('ì‚¬ëŒ', 'NNG'))

def kiwi_tokenize(text):
    split_s = kiwi.tokenize(text, stopwords=stopwords, normalize_coda=True)
    split_list = [i.form for i in split_s]
    split_f = ','.join(split_list).replace(",", " ")
    return split_f

def kiwi_word(text):
    split_s = kiwi.tokenize(text, stopwords=stopwords, normalize_coda=True)
    N_list = [i.form for i in split_s if i.tag == "NNG" or i.tag == "NNP"]
    return N_list

with open("bm25_retriever.pkl", "rb") as f:
    bm25_retriever = pickle.load(f)

with open("bm25_word.pkl", "rb") as f:
    bm25_word = pickle.load(f)

faiss_retriever = db.as_retriever(search_kwargs={"k": 5})


prompt_text = """
Your role is to recommend a place to eat something to the user.
Follow the instructions below to achieve your role.

1) Read the user's message and rephrase it in a standard language to understand the user's intention deeply.
2) Choose one that is most relevant to the user's intention from the list of restaurants.
Each of the restaurants consists of the 0. ê°€ê²Œëª…, 1. ë©”ë‰´ëª©ë¡, 2. ë¦¬ë·°ëª©ë¡. Carefully read the information. 
list of restaurants:
{context}
4) Answer the name of the chosen restaurant and the reason for the recommendation.
4-1) If you cannot find a suitable restaurant, you must choose a place not in the list of restaurants, but from the chat history.
Please try to match a correct menu.
4-2) If you can never find a suitable restaurant, encourage the user to ask in other ways.
4-3) If the user's message is not related to the place to eat, encourage the user to get a recommendation or rather, you suggest a menu to the user first.
5) Answer in Korean.
"""

bm25_retriever.invoke('.')
bm25_word.invoke('.')

def simplify_list(list, max_length):
    for i, store_data in enumerate(list):
        reviews_start = store_data.find("2. ë¦¬ë·°ëª©ë¡:") 
        if reviews_start != -1:
            reviews = eval(store_data[reviews_start + len("2. ë¦¬ë·°ëª©ë¡:"):])
            truncated_reviews = [review[:max_length] + "..." if len(review) > max_length else review for review in reviews]
            list[i] = (
                store_data[:reviews_start + len("2. ë¦¬ë·°ëª©ë¡:")] + str(truncated_reviews)
            )
    return list

# Streamlit UI ì„¤ì •
st.title("ğŸ´ í™ëŒ€ ê·¼ì²˜ ìŒì‹ì  ì¶”ì²œ ì±—ë´‡")
st.write("ìŒì‹ì  ì¶”ì²œì´ í•„ìš”í•˜ë©´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!")


def RAG_lists(text):
    bm_r = bm25_retriever.invoke(text)
    bm_w = bm25_word.invoke(text)
    faiss_r = faiss_retriever.invoke(text)

    def add_to_list(source, RAG_list, max_length, n=0):
        while len(RAG_list) < max_length and n < len(source):
            if source[n] not in RAG_list:
                RAG_list.append(source[n])
            n += 1
        return n

    RAG_list = []
    n = add_to_list(faiss_r, RAG_list, 4)  # faiss_rì—ì„œ ìµœëŒ€ 4ê°œ ì¶”ê°€
    if len(RAG_list) < 7:
        n = add_to_list(bm_r, RAG_list, 7, n)  # bm_rì—ì„œ ì¶”ê°€í•˜ì—¬ ìµœëŒ€ 7ê°œ ì±„ì›€
    if len(RAG_list) < 7:
        add_to_list(bm_w, RAG_list, 7)  # bm_wì—ì„œ ì¶”ê°€í•˜ì—¬ ìµœëŒ€ 7ê°œ ì±„ì›€

    result = [doc.page_content for doc in RAG_list]
    if "context_memory" not in st.session_state:
        st.session_state["context_memory"] = []
    st.session_state["context_memory"].append(result)
    if len(st.session_state["context_memory"]) > 3:
        st.session_state["context_memory"].pop(0)
    return result


llm = ChatOpenAI(model_name="gpt-4o", temperature=0.5, api_key=api_key)
rag_runnable = RunnableLambda(lambda inputs: RAG_lists(inputs))


def get_or_create_user_id():
    if "user_id" not in st.session_state:
        # ëœë¤ ID ìƒì„±
        st.session_state["user_id"] = str(uuid.uuid4())  # ê³ ìœ  UUID ìƒì„±
    return st.session_state["user_id"]

if "user_memories" not in st.session_state:
    st.session_state["user_memories"] = {}

def get_user_memory():
    user_id = get_or_create_user_id()
    if user_id not in st.session_state["user_memories"]:
        # ì‚¬ìš©ìë³„ ConversationBufferWindowMemory ìƒì„±
        st.session_state["user_memories"][user_id] = ConversationBufferWindowMemory(
            memory_key="history",
            return_messages=True,
            k=3  # ìµœê·¼ 3ê°œì˜ ëŒ€í™”ë§Œ ìœ ì§€
        )
    return st.session_state["user_memories"][user_id]

prompt_template = ChatPromptTemplate.from_messages(
        [
            ("system", prompt_text),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}"),
        ]
    )

user_memory = get_user_memory()
def load_memory(x):
    return user_memory.load_memory_variables({})["history"]


chain = (
    {"context": rag_runnable, "question": RunnablePassthrough()}
    | RunnablePassthrough.assign(history=load_memory)
    | prompt_template
    | llm
    | StrOutputParser()
)

def invoke_chain(question):
    result = chain.invoke(question)
    user_memory.save_context(
        inputs={"question": question},
        outputs={
            "response": result
            + "â–§ This was responsed by following RAG results: "
            + ", ".join(map(str, simplify_list(st.session_state["context_memory"][-1][:3], 40)))
        },
    )
    return result

# ë¹„ë™ê¸° í•¨ìˆ˜ (ì‚¬ìš©ìë³„ context_memory ê´€ë¦¬)
async def invoke_chain_async(question):
    # ì˜ˆì‹œë¡œ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
    result = await asyncio.to_thread(invoke_chain, question)
    # ì‚¬ìš©ì context_memory ì—…ë°ì´íŠ¸
    # st.session_state["context_memory"].append(
    #     {"question": question, "response": result}
    # )
    return result

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="user_input")

# Streamlitì˜ ë¹„ë™ê¸° ì§€ì› í™œìš©
if user_input:
    # ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬
    response = asyncio.run(invoke_chain_async(user_input))

    # UI ì—…ë°ì´íŠ¸
    st.text_area("ğŸ¤– ì±—ë´‡ì˜ ì‘ë‹µ", value=response, height=200)
    print(load_memory(""))

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    st.write("### ëŒ€í™” ê¸°ë¡")
    st.write(load_memory("")[0].content)
    split_a = load_memory("")[1].content.find("â–§")
    st.write(load_memory("")[1].content[:split_a])
    try:
        st.write(load_memory("")[2].content)
        split_b = load_memory("")[3].content.find("â–§")
        st.write(load_memory("")[3].content[:split_b])
    except IndexError:
        pass
    try:
        st.write(load_memory("")[4].content)
        split_c = load_memory("")[5].content.find("â–§")
        st.write(load_memory("")[5].content[:split_c])
    except IndexError:
        pass

# streamlit run main_test.py

# ê°€ìƒí™˜ê²½:
# C:\Users\nodap\OneDrive\Desktop\bigdata\study\.venv\Scripts\Activate.ps1